{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfaf5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 10:39:19.300372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-07 10:39:19.308246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770440959.317449   26668 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770440959.320196   26668 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770440959.327124   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327234   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327238   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327239   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-07 10:39:19.329592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "Input dtype: <class 'numpy.uint8'>\n",
      "Input quantization: 0.003921568859368563 0\n",
      "\n",
      "▶ Running INT8 inference benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saber/GitHub/road_anomaly_detection/.venv/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS ==========\n",
      "Frames measured : 280\n",
      "Avg latency     : 74.70 ms\n",
      "Avg FPS         : 13.39\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONFIG\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "IMG_SIZE = 640\n",
    "NUM_WARMUP = 20        # frames (do not count)\n",
    "MAX_FRAMES = 300       # cap for faster testing\n",
    "\n",
    "print(\"Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Quantization params\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "print(\"Input dtype:\", input_details[0][\"dtype\"])\n",
    "print(\"Input quantization:\", input_scale, input_zero_point)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "print(\"\\n Running INT8 inference benchmark...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    img = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize input\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# RESULTS\n",
    "total_frames = len(timings)\n",
    "avg_time = sum(timings) / total_frames\n",
    "avg_fps = 1.0 / avg_time\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames measured : {total_frames}\")\n",
    "print(f\"Avg latency     : {avg_time*1000:.2f} ms\")\n",
    "print(f\"Avg FPS         : {avg_fps:.2f}\")\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fef6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "Input dtype: <class 'numpy.uint8'>\n",
      "Input quantization: 0.003921568859368563 0\n",
      "\n",
      "▶ Running INT8 inference benchmark (downscaled decode)...\n",
      "\n",
      "========== RESULTS ==========\n",
      "Frames measured : 280\n",
      "Avg latency     : 75.17 ms\n",
      "Avg FPS         : 13.30\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONFIG\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "\n",
    "MODEL_IMG_SIZE = 640          # YOLO input\n",
    "VIDEO_DECODE_WIDTH = 960      # ⬅️ LOWER than 1920\n",
    "VIDEO_DECODE_HEIGHT = 540\n",
    "\n",
    "NUM_WARMUP = 20\n",
    "MAX_FRAMES = 300\n",
    "\n",
    "print(\"▶ Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "print(\"Input dtype:\", input_details[0][\"dtype\"])\n",
    "print(\"Input quantization:\", input_scale, input_zero_point)\n",
    "\n",
    "# Video capture with reduced resolution\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, VIDEO_DECODE_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, VIDEO_DECODE_HEIGHT)\n",
    "\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "print(\"\\n▶ Running INT8 inference benchmark (downscaled decode)...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    img = cv2.resize(frame, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# RESULTS\n",
    "total_frames = len(timings)\n",
    "avg_time = sum(timings) / total_frames\n",
    "avg_fps = 1.0 / avg_time\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames measured : {total_frames}\")\n",
    "print(f\"Avg latency     : {avg_time*1000:.2f} ms\")\n",
    "print(f\"Avg FPS         : {avg_fps:.2f}\")\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7261094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 14:11:31.277940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-10 14:11:31.413463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770712891.480986    8365 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770712891.500653    8365 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770712891.623689    8365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770712891.623720    8365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770712891.623724    8365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770712891.623726    8365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-10 14:11:31.640803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "\n",
      "▶ Running inference + saving output video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saber/GitHub/road_anomaly_detection/.venv/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS ==========\n",
      "Frames measured : 280\n",
      "Avg latency     : 181.14 ms\n",
      "Avg FPS         : 5.52\n",
      "Saved video     : ../../../runs/tflite/output_predictions.mp4\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ================= CONFIG =================\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "OUTPUT_VIDEO = \"../../../runs/tflite/output_predictions.mp4\"\n",
    "\n",
    "MODEL_IMG_SIZE = 640\n",
    "VIDEO_DECODE_WIDTH = 960\n",
    "VIDEO_DECODE_HEIGHT = 540\n",
    "\n",
    "CONF_THRESH = 0.25\n",
    "NUM_WARMUP = 20\n",
    "MAX_FRAMES = 300\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Road Defect\",\n",
    "    1: \"Pothole\"\n",
    "}\n",
    "\n",
    "# ================= LOAD MODEL =================\n",
    "print(\"▶ Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "# ================= VIDEO SETUP =================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, VIDEO_DECODE_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, VIDEO_DECODE_HEIGHT)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "writer = cv2.VideoWriter(\n",
    "    OUTPUT_VIDEO,\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps,\n",
    "    (VIDEO_DECODE_WIDTH, VIDEO_DECODE_HEIGHT)\n",
    ")\n",
    "\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "print(\"\\n▶ Running inference + saving output video...\")\n",
    "\n",
    "# ================= INFERENCE LOOP =================\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "\n",
    "    # ---------- PREPROCESS ----------\n",
    "    img = cv2.resize(frame, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "    # ---------- POSTPROCESS ----------\n",
    "    output = interpreter.get_tensor(output_details[0][\"index\"])[0]\n",
    "    output = output.transpose(1, 0)   # (8400, 6)\n",
    "\n",
    "\n",
    "    for det in output:\n",
    "        x1, y1, x2, y2, score, cls = det\n",
    "\n",
    "        if score < CONF_THRESH:\n",
    "            continue\n",
    "\n",
    "        cls = int(cls)\n",
    "\n",
    "        # Scale boxes back to original frame\n",
    "        x1 = int(x1 * orig_w / MODEL_IMG_SIZE)\n",
    "        x2 = int(x2 * orig_w / MODEL_IMG_SIZE)\n",
    "        y1 = int(y1 * orig_h / MODEL_IMG_SIZE)\n",
    "        y2 = int(y2 * orig_h / MODEL_IMG_SIZE)\n",
    "\n",
    "        label = f\"{CLASS_NAMES.get(cls, 'Unknown')} {score:.2f}\"\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            label,\n",
    "            (x1, max(20, y1 - 5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "# ================= RESULTS =================\n",
    "avg_time = sum(timings) / len(timings)\n",
    "avg_fps = 1.0 / avg_time\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames measured : {len(timings)}\")\n",
    "print(f\"Avg latency     : {avg_time*1000:.2f} ms\")\n",
    "print(f\"Avg FPS         : {avg_fps:.2f}\")\n",
    "print(f\"Saved video     : {OUTPUT_VIDEO}\")\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5ed5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "ret, test_frame = cap.read()\n",
    "assert ret, \"Could not read first frame\"\n",
    "\n",
    "FRAME_H, FRAME_W = test_frame.shape[:2]\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # rewind video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d62fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = cv2.VideoWriter(\n",
    "    OUTPUT_VIDEO.replace(\".mp4\", \".avi\"),\n",
    "    cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "    fps,\n",
    "    (FRAME_W, FRAME_H)\n",
    ")\n",
    "\n",
    "assert writer.isOpened(), \"❌ VideoWriter failed to open\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d54394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output details: [{'name': 'PartitionedCall:0', 'index': 419, 'shape': array([   1,    6, 8400], dtype=int32), 'shape_signature': array([   1,    6, 8400], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (2.5373830795288086, 0), 'quantization_parameters': {'scales': array([2.537383], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output details:\", output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dba9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (6, 8400)\n",
      "One detection vector length: (8400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", output.shape)\n",
    "print(\"One detection vector length:\", output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bc406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e9de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013eedc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "▶ Saving output video: ../../../runs/tflite/output_predictions.avi\n",
      "▶ Resolution: 1920x1080, FPS: 24.00\n",
      "\n",
      "========== RESULTS ==========\n",
      "Frames processed : 301\n",
      "Avg FPS (model)  : 4.47\n",
      "Output file     : ../../../runs/tflite/output_predictions.avi\n",
      "File size       : 40.93 MB\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# ================= CONFIG =================\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "OUTPUT_VIDEO = \"../../../runs/tflite/output_predictions.avi\"\n",
    "\n",
    "MODEL_IMG_SIZE = 640\n",
    "CONF_THRESH = 0.25\n",
    "NUM_WARMUP = 20\n",
    "MAX_FRAMES = 300\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"Road Defect\",\n",
    "    1: \"Pothole\"\n",
    "}\n",
    "\n",
    "# ================= LOAD MODEL =================\n",
    "print(\"▶ Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "# ================= VIDEO SETUP =================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"❌ Could not open input video\"\n",
    "\n",
    "# Read first frame to get REAL resolution\n",
    "ret, first_frame = cap.read()\n",
    "assert ret, \"❌ Could not read first frame\"\n",
    "\n",
    "FRAME_H, FRAME_W = first_frame.shape[:2]\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Reset video to first frame\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# Create output directory if needed\n",
    "os.makedirs(os.path.dirname(OUTPUT_VIDEO), exist_ok=True)\n",
    "\n",
    "# Safe codec for Linux / Raspberry Pi\n",
    "writer = cv2.VideoWriter(\n",
    "    OUTPUT_VIDEO,\n",
    "    cv2.VideoWriter_fourcc(*\"XVID\"),\n",
    "    fps,\n",
    "    (FRAME_W, FRAME_H)\n",
    ")\n",
    "\n",
    "assert writer.isOpened(), \"❌ VideoWriter failed to open\"\n",
    "\n",
    "print(f\"▶ Saving output video: {OUTPUT_VIDEO}\")\n",
    "print(f\"▶ Resolution: {FRAME_W}x{FRAME_H}, FPS: {fps:.2f}\")\n",
    "\n",
    "# ================= INFERENCE LOOP =================\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    orig_h, orig_w = frame.shape[:2]\n",
    "\n",
    "    # ---------- PREPROCESS ----------\n",
    "    img = cv2.resize(frame, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize to INT8\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "    # ---------- POSTPROCESS ----------\n",
    "    output = interpreter.get_tensor(output_details[0][\"index\"])[0]\n",
    "    output = output.transpose(1, 0)  # (8400, 6)\n",
    "\n",
    "    for det in output:\n",
    "        cx, cy, w, h, conf, cls = det\n",
    "\n",
    "        conf = conf / 255.0  # INT8 → float\n",
    "        if conf < CONF_THRESH:\n",
    "            continue\n",
    "\n",
    "        cls = int(cls)\n",
    "\n",
    "        # Convert center → corner\n",
    "        x1 = int((cx - w / 2) * orig_w / MODEL_IMG_SIZE)\n",
    "        y1 = int((cy - h / 2) * orig_h / MODEL_IMG_SIZE)\n",
    "        x2 = int((cx + w / 2) * orig_w / MODEL_IMG_SIZE)\n",
    "        y2 = int((cy + h / 2) * orig_h / MODEL_IMG_SIZE)\n",
    "\n",
    "        # Clamp to image bounds\n",
    "        x1 = max(0, min(x1, orig_w - 1))\n",
    "        y1 = max(0, min(y1, orig_h - 1))\n",
    "        x2 = max(0, min(x2, orig_w - 1))\n",
    "        y2 = max(0, min(y2, orig_h - 1))\n",
    "\n",
    "        label = f\"{CLASS_NAMES.get(cls, 'Unknown')} {conf:.2f}\"\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            label,\n",
    "            (x1, max(20, y1 - 5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    # ---------- WRITE FRAME ----------\n",
    "    writer.write(frame)\n",
    "\n",
    "# ================= CLEANUP =================\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "# ================= RESULTS =================\n",
    "if timings:\n",
    "    avg_time = sum(timings) / len(timings)\n",
    "    avg_fps = 1.0 / avg_time\n",
    "else:\n",
    "    avg_fps = 0.0\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames processed : {frame_count}\")\n",
    "print(f\"Avg FPS (model)  : {avg_fps:.2f}\")\n",
    "print(f\"Output file     : {OUTPUT_VIDEO}\")\n",
    "print(f\"File size       : {os.path.getsize(OUTPUT_VIDEO) / 1e6:.2f} MB\")\n",
    "print(\"================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
