{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfaf5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 10:39:19.300372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-07 10:39:19.308246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770440959.317449   26668 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770440959.320196   26668 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770440959.327124   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327234   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327238   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770440959.327239   26668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-07 10:39:19.329592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "Input dtype: <class 'numpy.uint8'>\n",
      "Input quantization: 0.003921568859368563 0\n",
      "\n",
      "▶ Running INT8 inference benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saber/GitHub/road_anomaly_detection/.venv/lib/python3.12/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RESULTS ==========\n",
      "Frames measured : 280\n",
      "Avg latency     : 74.70 ms\n",
      "Avg FPS         : 13.39\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONFIG\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "IMG_SIZE = 640\n",
    "NUM_WARMUP = 20        # frames (do not count)\n",
    "MAX_FRAMES = 300       # cap for faster testing\n",
    "\n",
    "print(\"Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Quantization params\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "print(\"Input dtype:\", input_details[0][\"dtype\"])\n",
    "print(\"Input quantization:\", input_scale, input_zero_point)\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "print(\"\\n Running INT8 inference benchmark...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    img = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize input\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# RESULTS\n",
    "total_frames = len(timings)\n",
    "avg_time = sum(timings) / total_frames\n",
    "avg_fps = 1.0 / avg_time\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames measured : {total_frames}\")\n",
    "print(f\"Avg latency     : {avg_time*1000:.2f} ms\")\n",
    "print(f\"Avg FPS         : {avg_fps:.2f}\")\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fef6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading TFLite INT8 model...\n",
      "Input dtype: <class 'numpy.uint8'>\n",
      "Input quantization: 0.003921568859368563 0\n",
      "\n",
      "▶ Running INT8 inference benchmark (downscaled decode)...\n",
      "\n",
      "========== RESULTS ==========\n",
      "Frames measured : 280\n",
      "Avg latency     : 75.17 ms\n",
      "Avg FPS         : 13.30\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONFIG\n",
    "TFLITE_MODEL = \"/home/saber/GitHub/road_anomaly_detection/runs/detect/yolov8s_rdd2022_2class7/weights/best_int8.tflite\"\n",
    "VIDEO_PATH = \"/home/saber/GitHub/road_anomaly_detection/data/videos/3695999-hd_1920_1080_24fps.mp4\"\n",
    "\n",
    "MODEL_IMG_SIZE = 640          # YOLO input\n",
    "VIDEO_DECODE_WIDTH = 960      # ⬅️ LOWER than 1920\n",
    "VIDEO_DECODE_HEIGHT = 540\n",
    "\n",
    "NUM_WARMUP = 20\n",
    "MAX_FRAMES = 300\n",
    "\n",
    "print(\"▶ Loading TFLite INT8 model...\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "print(\"Input dtype:\", input_details[0][\"dtype\"])\n",
    "print(\"Input quantization:\", input_scale, input_zero_point)\n",
    "\n",
    "# Video capture with reduced resolution\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "assert cap.isOpened(), \"Could not open video\"\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, VIDEO_DECODE_WIDTH)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, VIDEO_DECODE_HEIGHT)\n",
    "\n",
    "frame_count = 0\n",
    "timings = []\n",
    "\n",
    "print(\"\\n▶ Running INT8 inference benchmark (downscaled decode)...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > MAX_FRAMES:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    img = cv2.resize(frame, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize\n",
    "    img = img / input_scale + input_zero_point\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    interpreter.invoke()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if frame_count > NUM_WARMUP:\n",
    "        timings.append(end - start)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# RESULTS\n",
    "total_frames = len(timings)\n",
    "avg_time = sum(timings) / total_frames\n",
    "avg_fps = 1.0 / avg_time\n",
    "\n",
    "print(\"\\n========== RESULTS ==========\")\n",
    "print(f\"Frames measured : {total_frames}\")\n",
    "print(f\"Avg latency     : {avg_time*1000:.2f} ms\")\n",
    "print(f\"Avg FPS         : {avg_fps:.2f}\")\n",
    "print(\"================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road_anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
